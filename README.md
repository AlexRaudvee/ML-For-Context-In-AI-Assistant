
# ğŸ§­ Embeddings-based Code Search (CoSQA)

A minimal-but-extendable code search engine with:
- **Part 1**: Embeddings-based search + REST API (FastAPI)
- **Part 2**: Evaluation on **CoSQA** with Recall@10, MRR@10, nDCG@10
- **Part 3**: Fine-tuning on CoSQA to improve metrics (contrastive bi-encoder)
- **Bonus**: Function name vs body search, index hyperparam sweeps


---

## ğŸ”§ Quickstart

```bash
# 1) Create env and install deps
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 2) Ingest a small toy corpus (or your own code base)
python scripts/ingest.py --input_glob "data/examples/**/*.py" --index_path models/usearch.index

# 3) Serve the search API
python scripts/serve_api.py --index_path models/usearch.index --model sentence-transformers/all-MiniLM-L6-v2

# 4) Query
curl -X POST "http://localhost:8000/search" -H "Content-Type: application/json" -d '{"query":"read a csv file in pandas", "k": 5}'
```

---

## ğŸ—‚ï¸ Repository structure

```
.
â”œâ”€â”€ README.md                  # how to run each part, milestones
â”œâ”€â”€ requirements.txt           # deps: sentence-transformers, usearch, etc.
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml           # model/index/train params
â”œâ”€â”€ data/                      # raw datasets (gitignored)
â”œâ”€â”€ models/                    # saved indexes & finetuned models (gitignored)
â”œâ”€â”€ results/                   # eval outputs & plots (gitignored)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ report.ipynb           # final report notebook (fill as you go)
â”œâ”€â”€ scripts/                   # CLI entry points (no business logic)
â”‚   â”œâ”€â”€ ingest.py              # build an index from a code corpus
â”‚   â”œâ”€â”€ serve_api.py           # run FastAPI server
â”‚   â”œâ”€â”€ evaluate.py            # run eval (CoSQA) + print metrics
â”‚   â””â”€â”€ train_biencoder.py     # finetune bi-encoder on CoSQA
â””â”€â”€ src/codesearch/
    â”œâ”€â”€ api/
    â”‚   â””â”€â”€ main.py            # FastAPI app + /search endpoint
    â”œâ”€â”€ data/
    â”‚   â””â”€â”€ cosqa.py           # CoSQA loading/prep
    â”œâ”€â”€ eval/
    â”‚   â””â”€â”€ evaluator.py       # evaluation driver
    â”œâ”€â”€ index/
    â”‚   â”œâ”€â”€ base.py            # VectorIndex interface
    â”‚   â””â”€â”€ usearch_index.py   # USearch backend (add/search/save/load)
    â”œâ”€â”€ metrics/
    â”‚   â””â”€â”€ ranking.py         # Recall@10, MRR@10, nDCG@10
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ losses.py          # MultipleNegatives & InfoNCE losses
    â”‚   â””â”€â”€ trainer.py         # simple finetuning loop (PyTorch)
    â”œâ”€â”€ utils/
    â”‚   â””â”€â”€ logging.py
    â””â”€â”€ embeddings.py          # SBERT wrapper (encode texts)
```

<!-- --- -->

<!-- ## âœ… Milestones & what to implement -->

<!-- ### Part 1 â€” Search Engine
- [ ] `codesearch/embeddings.py`: wrap a HuggingFace/SBERT model (encode texts)
- [ ] `codesearch/index/base.py`: `VectorIndex` interface (add, finalize, search, save/load)
- [ ] `codesearch/index/usearch_index.py`: minimal in-memory ANN via `usearch`
- [ ] `scripts/ingest.py`: read a corpus (e.g., .py files), chunk to passages, build index
- [ ] `src/codesearch/api/main.py` + `scripts/serve_api.py`: FastAPI `/search`

### Part 2 â€” Evaluation
- [ ] `codesearch/data/cosqa.py`: download/prepare CoSQA pairs
- [ ] `codesearch/metrics/ranking.py`: Recall@10, MRR@10, nDCG@10
- [ ] `codesearch/eval/evaluator.py` & `scripts/evaluate.py`: compute metrics

### Part 3 â€” Fine-tuning
- [ ] `codesearch/train/losses.py`: Contrastive/MultipleNegativesRankingLoss or InfoNCE
- [ ] `codesearch/train/trainer.py` & `scripts/train_biencoder.py`: train textâ†”code bi-encoder
- [ ] Log training loss; save plots to `results/`

### Bonus
- [ ] Function names vs full bodies: a flag in dataset prep/eval
- [ ] Index hyperparam sweeps: `ef`, `metric`, `quantization`, `connectivity` -->

---

## ğŸ““ Report Notebook

In `notebooks/report.ipynb`:
- shown some sample queries & results
- computed metrics (baseline vs fine-tuned)
- includes loss curve plot
- briefly justifies **loss choice** and shows **hyperparam impacts**

---

## ğŸ“œ License

MIT License
